{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "game_url = \"game/dino.html\"\n",
    "chrome_driver_path = \"../chromedriver.exe\"\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(200, 300)\n",
    "        self._driver.get(os.path.abspath(game_url))\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        return self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game;\n",
    "        self.jump();\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing image as required\n",
    "def process_img(image):\n",
    "    #image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)qqq\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200)\n",
    "    #image = resized_image = cv2.resize(image, (80, 80)) \n",
    "    image = cv2.resize(image, (0,0), fx = 0.50, fy = 0.50)\n",
    "    #image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
    "    return  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_screen():\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(0,180,400,400)))\n",
    "    image = process_img(screen)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 200)\n"
     ]
    }
   ],
   "source": [
    "print(grab_screen().shape)\n",
    "# game = Game()\n",
    "# dino = DinoAgent(game)\n",
    "# last_time = time.time()\n",
    "# while(True):\n",
    "    \n",
    "# #     print('loop took {} seconds'.format(time.time()-last_time))\n",
    "# #     last_time = time.time()\n",
    "# #     cv2.imwrite(\"./img_data/dino\"+str(time())+\".jpg\",image)\n",
    "# #     dino.duck()\n",
    "#     #exit on q pres\n",
    "#     image,r_t,end_t = get_state(game,dino,2)\n",
    "# #     print('{0} {1} '.format(r_t,end_t))\n",
    "#     #cv2.imshow('window',image)\n",
    "#     if(dino.is_crashed()):\n",
    "#         #jumping starts the game again if dino has crashed\n",
    "#         print(game.get_score())\n",
    "#         game.restart()\n",
    "        \n",
    "#     if (cv2.waitKey(25) & 0xFF == ord('q')):\n",
    "#         cv2.destroyAllWindows()\n",
    "#         game.end()\n",
    "#         cv2.imwrite('dino.jpg',image)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 3 # possible actions: jump, duck , do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 3200. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows , img_cols = 200, 110\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(3))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model,game_state):\n",
    "    # open up a game state to communicate with emulator\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "    display = show_img()\n",
    "    display.__next__()\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    \n",
    "\n",
    "    #In Keras, need to reshape\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
    "\n",
    "    print (\"this is the shape\"+str(s_t.shape))\n",
    "\n",
    "    if False :#args['mode'] == 'Run':\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "\n",
    "    t = 0\n",
    "    while (True):\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "                a_t[action_index] = 1\n",
    "\n",
    "        #We reduced the epsilon gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        display.send(x_t1)\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "\n",
    "\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 80, 80, 4\n",
    "            print (inputs.shape)\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]\n",
    "                state_t1 = minibatch[i][3]\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminated, only equals reward\n",
    "\n",
    "                inputs[i:i + 1] = state_t    #I saved down s_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # Hitting each buttom probability\n",
    "                Q_sa = model.predict(state_t1)\n",
    "\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            # targets2 = normalize(targets)\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "\n",
    "        s_t = s_t1\n",
    "        t = t + 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playGame():\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)\n",
    "    model = buildmodel()\n",
    "    trainNetwork(model,game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "    def get_state(self,actions):\n",
    "        \n",
    "        reward = 1\n",
    "        is_over = False\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = 0.1\n",
    "        elif (actions[2] == 1):\n",
    "            self._agent.duck()\n",
    "            reward = 0.1\n",
    "        if self._agent.is_crashed():\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        image = grab_screen()\n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(110, 200,..., strides=(4, 4), padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\ravi7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\ravi7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finish building the model\n",
      "this is the shape(1, 110, 200, 4)\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 284 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 285 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 286 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 287 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 288 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 289 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 290 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 291 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 292 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 293 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 294 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 295 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 296 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 297 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 298 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 299 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 300 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 301 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 302 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 303 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 304 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7bf0dcea9dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-7d42e0357155>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgame_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame_sate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdino\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-495937fcf2da>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mx_t1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_t1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#1x80x80x1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0ms_t1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_img():\n",
    "    \"\"\"\n",
    "    Coroutine to store images in the \"images\" directory\n",
    "    \"\"\"\n",
    "    frame = 0\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        cv2.imshow(\"preview\", screen)\n",
    "        if (cv2.waitKey(25) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

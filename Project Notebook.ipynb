{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from IPython.display import clear_output\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "game_url = \"game/dino.html\"\n",
    "chrome_driver_path = \"../chromedriver.exe\"\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(200, 300)\n",
    "        self._driver.get(os.path.abspath(game_url))\n",
    "        #modifying game before trainNetworkining\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "#         self._driver.execute_script(\"Runner.instance_.config.CLEAR_TIME = \"+ str(randint(1, 999)))\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "#         self._driver.refresh()\n",
    "#         self.press_up()\n",
    "        time.sleep(0.25)\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game;\n",
    "        self.jump();\n",
    "        time.sleep(.5)\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing image as required\n",
    "def process_img(image):\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "#     image = cv2.Canny(image, threshold1 = 100, threshold2 = 200)\n",
    "#     image = image[10:140,0:200] #img[y:y+h, x:x+w]\n",
    "#     image = resized_image = cv2.resize(image, (80, 80)) \n",
    "    image = cv2.resize(image, (0,0), fx = 0.15, fy = 0.10)\n",
    "    image = image[2:38,10:50] #img[y:y+h, x:x+w]\n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200)\n",
    "    return  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_screen(_driver = None):\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(40,180,440,400)))\n",
    "    image = process_img(screen)\n",
    "    if _driver!=None:\n",
    "        image = _driver.get_screenshot_as_png()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(grab_screen().shape)\n",
    "# # game = Game()\n",
    "# # dino = DinoAgent(game)\n",
    "# # last_time = time.time()\n",
    "# while(True):\n",
    "    \n",
    "# # # #     print('loop took {} seconds'.format(time.time()-last_time))\n",
    "# # # #     last_time = time.time()\n",
    "# # # #     cv2.imwrite(\"./img_data/dino\"+str(time())+\".jpg\",image)\n",
    "# # # #     dino.duck()\n",
    "# # #     #exit on q pres\n",
    "# # # #     print('{0} {1} '.format(r_t,end_t))\n",
    "# # # #     cv2.imshow('window',game.grab_screen())\n",
    "#     image = grab_screen()\n",
    "#     cv2.imshow('window',image)\n",
    "    \n",
    "\n",
    "# # # #     from matplotlib import pyplot as plt\n",
    "# # # #     plt.imshow(image)\n",
    "# # # #     plt.title('my picture')\n",
    "# # # #     plt.show()\n",
    "\n",
    "# # # #     grab_screen()\n",
    "# # #     if(dino.is_crashed()):\n",
    "# # #         #jumping starts the game again if dino has crashed\n",
    "# # # #         temp = (game.get_score())\n",
    "# # #         game.restart()\n",
    "#     if (cv2.waitKey(25) & 0xFF == ord('q')):\n",
    "#         cv2.destroyAllWindows()\n",
    "# #         game.end()\n",
    "# #         cv2.imwrite('dino.jpg',image)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 50000. # timesteps to observe before training\n",
    "EXPLORE = 100000 #300000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"initial variable caching, done only once\"\"\"\n",
    "# save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "# t = 0\n",
    "# save_obj(t,\"time\")\n",
    "# # D = deque()\n",
    "# # save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows , img_cols = 40,20\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "#     Conv2D(32, (8, 8), strides=(4, 4), input_shape=(20, 40, 4..., padding=\"same\")`\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(model,game_state,observe=False):\n",
    "    # open up a game state to communicate with emulator\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #deque()\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    \n",
    "\n",
    "    #In Keras, need to reshape\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
    "    initial_state = s_t\n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\")#FINAL_EPSILON #INITIAL_EPSILON\n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\")\n",
    "    while (True):\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if  random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[0] = 1\n",
    "            else:\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "                a_t[action_index] = 1\n",
    "        #We reduced the epsilon gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE #update to original asap\n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('loop took {} seconds'.format(time.time()-last_time))\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: # and terminal: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]\n",
    "                state_t1 = minibatch[i][3]\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminated, only equals reward\n",
    "\n",
    "                inputs[i:i + 1] = state_t    #I saved down s_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # Hitting each buttom probability\n",
    "                Q_sa = model.predict(state_t1)\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "            # targets2 = normalize(targets)\n",
    "            loss = model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_max_df[len(q_max_df)] = np.max(Q_sa)\n",
    "        else:\n",
    "#             artificial time delay as training done with this delay\n",
    "#             loss_df.loc[len(loss_df)] = 0\n",
    "            time.sleep(0.20)\n",
    "        s_t = initial_state if terminal else s_t1\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 10000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            \n",
    "            model.save_weights(\"model_final.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_max_df.to_csv(\"./objects/q_values.csv\",index=False)\n",
    "            # clear_output()\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_df = pd.read_csv(\"./objects/loss_df.csv\")\n",
    "scores_df = pd.read_csv(\"./objects/scores_df.csv\")\n",
    "actions_df = pd.read_csv(\"./objects/actions_df.csv\")\n",
    "q_max_df = pd.read_csv(\"./objects/q_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)\n",
    "    model = buildmodel()\n",
    "    trainNetwork(model,game_state,observe=observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img()\n",
    "        self._display.__next__()\n",
    "        self._min_score=11\n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1]\n",
    "        score = self._game.get_score()\n",
    "        reward = 0.1*score/10\n",
    "        is_over = False\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = 0.1*score/11\n",
    "        image = grab_screen()\n",
    "        #self._display.send(image)\n",
    "\n",
    "        if self._agent.is_crashed():\n",
    "           \n",
    "            scores_df.loc[len(loss_df)] = score\n",
    "            self._game.restart()\n",
    "            reward = -11/score\n",
    "            is_over = True\n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        # Create window with freedom of dimensions\n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "#         cv2.imwrite(\"screenshot\"+str(frame)+\".png\",screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n",
      "loop took 0.5999703407287598 seconds\n",
      "TIMESTEP 1649001 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  5.18138 / Loss  1.1652\n",
      "loop took 0.8320727348327637 seconds\n",
      "TIMESTEP 1649002 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  5.62636 / Loss  0.384749\n",
      "loop took 0.20400047302246094 seconds\n",
      "TIMESTEP 1649003 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.11000000000000001 / Q_MAX  5.57935 / Loss  0.223876\n",
      "loop took 0.21156525611877441 seconds\n",
      "TIMESTEP 1649004 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.12000000000000002 / Q_MAX  5.96516 / Loss  0.718496\n",
      "loop took 0.23194026947021484 seconds\n",
      "TIMESTEP 1649005 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.11818181818181818 / Q_MAX  7.1218 / Loss  0.608331\n",
      "loop took 0.23126792907714844 seconds\n",
      "TIMESTEP 1649006 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  6.41713 / Loss  0.281193\n",
      "loop took 0.23200011253356934 seconds\n",
      "TIMESTEP 1649007 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.14545454545454548 / Q_MAX  3.28746 / Loss  0.378865\n",
      "loop took 0.22000336647033691 seconds\n",
      "TIMESTEP 1649008 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.17 / Q_MAX  6.51226 / Loss  0.427396\n",
      "loop took 0.20000481605529785 seconds\n",
      "TIMESTEP 1649009 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.19 / Q_MAX  3.76321 / Loss  1.29205\n",
      "loop took 0.21744275093078613 seconds\n",
      "TIMESTEP 1649010 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.2 / Q_MAX  5.29068 / Loss  0.342336\n",
      "loop took 0.23200774192810059 seconds\n",
      "TIMESTEP 1649011 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  5.56965 / Loss  1.01973\n",
      "loop took 0.24617791175842285 seconds\n",
      "TIMESTEP 1649012 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.2090909090909091 / Q_MAX  6.53368 / Loss  0.35856\n",
      "loop took 0.2324512004852295 seconds\n",
      "TIMESTEP 1649013 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.21818181818181823 / Q_MAX  5.57604 / Loss  0.449129\n",
      "loop took 0.2171621322631836 seconds\n",
      "TIMESTEP 1649014 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.22727272727272727 / Q_MAX  5.32561 / Loss  0.951786\n",
      "loop took 0.2011733055114746 seconds\n",
      "TIMESTEP 1649015 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.27 / Q_MAX  7.56317 / Loss  0.543141\n",
      "loop took 0.2569155693054199 seconds\n",
      "TIMESTEP 1649016 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.2545454545454546 / Q_MAX  3.95398 / Loss  0.235733\n",
      "loop took 0.2239985466003418 seconds\n",
      "TIMESTEP 1649017 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.2727272727272727 / Q_MAX  5.52932 / Loss  0.596195\n",
      "loop took 0.23456215858459473 seconds\n",
      "TIMESTEP 1649018 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.2818181818181818 / Q_MAX  6.79456 / Loss  1.00507\n",
      "loop took 0.20098352432250977 seconds\n",
      "TIMESTEP 1649019 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.32 / Q_MAX  5.14742 / Loss  0.550255\n",
      "loop took 0.23199892044067383 seconds\n",
      "TIMESTEP 1649020 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.30909090909090914 / Q_MAX  4.43267 / Loss  0.967509\n",
      "loop took 0.20000195503234863 seconds\n",
      "TIMESTEP 1649021 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.35 / Q_MAX  5.59591 / Loss  1.04692\n",
      "loop took 0.2519993782043457 seconds\n",
      "TIMESTEP 1649022 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.32727272727272727 / Q_MAX  6.93765 / Loss  0.648545\n",
      "loop took 0.21599984169006348 seconds\n",
      "TIMESTEP 1649023 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.34545454545454546 / Q_MAX  6.1517 / Loss  0.722615\n",
      "loop took 0.2144618034362793 seconds\n",
      "TIMESTEP 1649024 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.39 / Q_MAX  4.80138 / Loss  0.25496\n",
      "loop took 0.2181549072265625 seconds\n",
      "TIMESTEP 1649025 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.4 / Q_MAX  7.36727 / Loss  1.18831\n",
      "loop took 0.21793127059936523 seconds\n",
      "TIMESTEP 1649026 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.42000000000000004 / Q_MAX  4.9933 / Loss  0.8039\n",
      "loop took 0.2160053253173828 seconds\n",
      "TIMESTEP 1649027 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.43 / Q_MAX  7.42502 / Loss  0.491678\n",
      "loop took 0.2316727638244629 seconds\n",
      "TIMESTEP 1649028 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.4 / Q_MAX  4.86982 / Loss  1.60454\n",
      "loop took 0.23599886894226074 seconds\n",
      "TIMESTEP 1649029 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.4181818181818182 / Q_MAX  6.23236 / Loss  1.15147\n",
      "loop took 0.23200488090515137 seconds\n",
      "TIMESTEP 1649030 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.4272727272727273 / Q_MAX  6.9012 / Loss  1.52634\n",
      "loop took 0.24840497970581055 seconds\n",
      "TIMESTEP 1649031 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.43636363636363645 / Q_MAX  5.81347 / Loss  0.296468\n",
      "loop took 0.19600152969360352 seconds\n",
      "TIMESTEP 1649032 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.5 / Q_MAX  3.70222 / Loss  0.469507\n",
      "loop took 0.21399188041687012 seconds\n",
      "TIMESTEP 1649033 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.51 / Q_MAX  5.25976 / Loss  1.14398\n",
      "loop took 0.23199963569641113 seconds\n",
      "TIMESTEP 1649034 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.4727272727272727 / Q_MAX  5.67696 / Loss  0.860496\n",
      "loop took 0.3840017318725586 seconds\n",
      "TIMESTEP 1649035 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.5 / Q_MAX  6.55979 / Loss  0.703448\n",
      "loop took 0.23199987411499023 seconds\n",
      "TIMESTEP 1649036 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.5090909090909091 / Q_MAX  6.06951 / Loss  0.649686\n",
      "loop took 0.23311400413513184 seconds\n",
      "TIMESTEP 1649037 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.5181818181818182 / Q_MAX  5.77714 / Loss  0.399234\n",
      "loop took 0.2160017490386963 seconds\n",
      "TIMESTEP 1649038 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.5900000000000001 / Q_MAX  7.54014 / Loss  0.680047\n",
      "loop took 0.2520003318786621 seconds\n",
      "TIMESTEP 1649039 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.5454545454545454 / Q_MAX  5.49979 / Loss  0.73348\n",
      "loop took 0.21438097953796387 seconds\n",
      "TIMESTEP 1649040 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.5636363636363636 / Q_MAX  6.25048 / Loss  0.834439\n",
      "loop took 0.21599817276000977 seconds\n",
      "TIMESTEP 1649041 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.6300000000000001 / Q_MAX  7.07102 / Loss  1.10385\n",
      "loop took 0.21600031852722168 seconds\n",
      "TIMESTEP 1649042 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.64 / Q_MAX  6.83578 / Loss  0.717957\n",
      "loop took 0.23200058937072754 seconds\n",
      "TIMESTEP 1649043 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.6000000000000001 / Q_MAX  5.30718 / Loss  1.22541\n",
      "loop took 0.2040095329284668 seconds\n",
      "TIMESTEP 1649044 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.67 / Q_MAX  7.0411 / Loss  1.17017\n",
      "loop took 0.2319943904876709 seconds\n",
      "TIMESTEP 1649045 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.6181818181818183 / Q_MAX  6.82461 / Loss  0.533299\n",
      "loop took 0.2319960594177246 seconds\n",
      "TIMESTEP 1649046 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.6363636363636364 / Q_MAX  7.27465 / Loss  0.430876\n",
      "loop took 0.21711421012878418 seconds\n",
      "TIMESTEP 1649047 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.7100000000000001 / Q_MAX  7.68258 / Loss  0.885101\n",
      "loop took 0.232987642288208 seconds\n",
      "TIMESTEP 1649048 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.72 / Q_MAX  5.26273 / Loss  0.344965\n",
      "loop took 0.2160027027130127 seconds\n",
      "TIMESTEP 1649049 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.74 / Q_MAX  7.36455 / Loss  0.329963\n",
      "loop took 0.23999786376953125 seconds\n",
      "TIMESTEP 1649050 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.6818181818181818 / Q_MAX  6.42194 / Loss  0.687385\n",
      "loop took 0.24000120162963867 seconds\n",
      "TIMESTEP 1649051 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.6909090909090909 / Q_MAX  4.19252 / Loss  0.592071\n",
      "loop took 0.19999957084655762 seconds\n",
      "TIMESTEP 1649052 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.78 / Q_MAX  4.03451 / Loss  0.179919\n",
      "loop took 0.2340857982635498 seconds\n",
      "TIMESTEP 1649053 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.79 / Q_MAX  6.93655 / Loss  0.650953\n",
      "loop took 0.21622848510742188 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 1649054 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.8 / Q_MAX  6.66267 / Loss  1.03409\n",
      "loop took 0.21600008010864258 seconds\n",
      "TIMESTEP 1649055 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.8200000000000001 / Q_MAX  7.54385 / Loss  0.695837\n",
      "loop took 0.23192620277404785 seconds\n",
      "TIMESTEP 1649056 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.7545454545454546 / Q_MAX  6.84024 / Loss  1.30695\n",
      "loop took 0.24212884902954102 seconds\n",
      "TIMESTEP 1649057 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.7727272727272727 / Q_MAX  6.78525 / Loss  0.889958\n",
      "loop took 0.2050333023071289 seconds\n",
      "TIMESTEP 1649058 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.86 / Q_MAX  7.83355 / Loss  0.55949\n",
      "loop took 0.21800780296325684 seconds\n",
      "TIMESTEP 1649059 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.8700000000000001 / Q_MAX  6.39915 / Loss  0.361035\n",
      "loop took 0.23199963569641113 seconds\n",
      "TIMESTEP 1649060 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8090909090909091 / Q_MAX  5.20218 / Loss  0.358237\n",
      "loop took 0.23201489448547363 seconds\n",
      "TIMESTEP 1649061 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8181818181818182 / Q_MAX  6.1459 / Loss  0.380425\n",
      "loop took 0.23280930519104004 seconds\n",
      "TIMESTEP 1649062 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8272727272727273 / Q_MAX  7.23392 / Loss  0.459227\n",
      "loop took 0.21596884727478027 seconds\n",
      "TIMESTEP 1649063 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8454545454545456 / Q_MAX  6.10244 / Loss  0.385239\n",
      "loop took 0.23137807846069336 seconds\n",
      "TIMESTEP 1649064 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8545454545454546 / Q_MAX  5.10783 / Loss  0.337512\n",
      "loop took 0.23399090766906738 seconds\n",
      "TIMESTEP 1649065 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.8636363636363636 / Q_MAX  2.54841 / Loss  0.241975\n",
      "loop took 0.23294806480407715 seconds\n",
      "TIMESTEP 1649066 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.881818181818182 / Q_MAX  7.52062 / Loss  1.06789\n",
      "loop took 0.23157954216003418 seconds\n",
      "TIMESTEP 1649067 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.890909090909091 / Q_MAX  7.03804 / Loss  0.81986\n",
      "loop took 0.2160015106201172 seconds\n",
      "TIMESTEP 1649068 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 1.0 / Q_MAX  7.49565 / Loss  0.859409\n",
      "loop took 0.367999792098999 seconds\n",
      "TIMESTEP 1649069 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.9090909090909091 / Q_MAX  7.10213 / Loss  0.484354\n",
      "loop took 0.21793818473815918 seconds\n",
      "TIMESTEP 1649070 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 1.0 / Q_MAX  2.32054 / Loss  1.05531\n",
      "loop took 0.24799895286560059 seconds\n",
      "TIMESTEP 1649071 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.9090909090909091 / Q_MAX  6.68468 / Loss  0.586978\n",
      "loop took 0.49881911277770996 seconds\n",
      "TIMESTEP 1649072 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD -0.10377358490566038 / Q_MAX  4.11842 / Loss  0.64759\n",
      "loop took 0.2518908977508545 seconds\n",
      "TIMESTEP 1649073 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.02727272727272728 / Q_MAX  4.36749 / Loss  0.225175\n",
      "loop took 0.21199893951416016 seconds\n",
      "TIMESTEP 1649074 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  2.71948 / Loss  0.346356\n",
      "loop took 0.23200106620788574 seconds\n",
      "TIMESTEP 1649075 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.045454545454545456 / Q_MAX  6.54709 / Loss  1.59178\n",
      "loop took 0.23600387573242188 seconds\n",
      "TIMESTEP 1649076 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.06363636363636364 / Q_MAX  3.68197 / Loss  0.37174\n",
      "loop took 0.21609783172607422 seconds\n",
      "TIMESTEP 1649077 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.08 / Q_MAX  5.24287 / Loss  0.252142\n",
      "loop took 0.21599888801574707 seconds\n",
      "TIMESTEP 1649078 / STATE train / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  6.20278 / Loss  1.11143\n",
      "loop took 0.25065088272094727 seconds\n",
      "TIMESTEP 1649079 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  5.42113 / Loss  0.500569\n",
      "loop took 0.2325747013092041 seconds\n",
      "TIMESTEP 1649080 / STATE train / EPSILON 0.0001 / ACTION 1 / REWARD 0.10909090909090911 / Q_MAX  5.40991 / Loss  1.17153\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=65.0.3325.181)\n  (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.16299 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a55f94520113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#searchforplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-2255e85230c6>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgame_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame_sate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdino\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-5b5723440b82>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loop took {} seconds'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-1cd2debe0429>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mactions_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mis_over\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4524d590e488>\u001b[0m in \u001b[0;36mget_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mARROW_DOWN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return Runner.instance_.distanceMeter.digits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    625\u001b[0m         return self.execute(command, {\n\u001b[0;32m    626\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    314\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'alert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=65.0.3325.181)\n  (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.16299 x86_64)\n"
     ]
    }
   ],
   "source": [
    "#searchforplay\n",
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_plots(realtime = True,t=0):\n",
    "    fig, axs = plt.subplots(ncols=2,nrows =2)\n",
    "    loss_df = pd.read_csv(\"./objects/loss_df.csv\")\n",
    "    scores_df = pd.read_csv(\"./objects/scores_df.csv\")\n",
    "    actions_df = pd.read_csv(\"./objects/actions_df.csv\")\n",
    "    q_max_df = pd.read_csv(\"./objects/q_values.csv\")\n",
    "    loss_df['loss'] = loss_df['loss'].astype('float') \n",
    "    loss_df.plot(use_index=True,ax=axs[0,0])\n",
    "    scores_df.plot(ax=axs[0,1])\n",
    "    sns.distplot(actions_df.tail(20000),ax=axs[1,0])\n",
    "#     q_max_df.plot(ax = axs[1,1])\n",
    "    imgg = fig.canvas.draw()\n",
    "    graph_img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    graph_img = graph_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "#     disp = show_img(graphs=True)\n",
    "#     disp.__next__()\n",
    "    cv2.imwrite(\"logs/progress/pg\"+str(t)+\".png\",graph_img) if realtime else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravi7\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    }
   ],
   "source": [
    "show_plots(realtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_df.tail(1000).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
